{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('retro': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9005f76a580ce8d3f10bba8d3bba0def2104943abe2739cd4ca70b38154e3b34"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "from ray.tune import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-02-06 14:55:31,500\tINFO services.py:1171 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "info = ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import retro_wrappers\n",
    "\n",
    "import retro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retro_env_creator(game, state):\n",
    "    base = retro.make(game=game, state=state)\n",
    "    base = retro_wrappers.wrap_megaman(base)\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_retro(game, state):\n",
    "    env_creator = lambda env_config: retro_env_creator(game, state)\n",
    "    register_env(game, env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config = DEFAULT_CONFIG.copy()\n",
    "trainer_config['framework'] = 'torch'\n",
    "trainer_config['num_workers'] = 1\n",
    "trainer_config[\"train_batch_size\"] = 400\n",
    "trainer_config[\"sgd_minibatch_size\"] = 64\n",
    "trainer_config[\"num_sgd_iter\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, config=trainer_config, checkpoint=None, iterations=1000000):\n",
    "    agent = PPOTrainer(config=config, env=env)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        result = agent.train()\n",
    "        print(pretty_print(result))\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            checkpoint = agent.save()\n",
    "            print(\"checkpoint salvo em\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_retro(\"MegaMan2-Nes\", \"Normal.Metalman.Fight.state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "22378968\n",
      "  mean_raw_obs_processing_ms: 0.24782315523846066\n",
      "time_since_restore: 23036.13481926918\n",
      "time_this_iter_s: 77.75088739395142\n",
      "time_total_s: 23036.13481926918\n",
      "timers:\n",
      "  learn_throughput: 6.785\n",
      "  learn_time_ms: 58949.584\n",
      "  sample_throughput: 76.557\n",
      "  sample_time_ms: 5224.832\n",
      "  update_time_ms: 5.815\n",
      "timestamp: 1612657196\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 158400\n",
      "training_iteration: 396\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2021-02-06_21-21-13\n",
      "done: false\n",
      "episode_len_mean: 368.96\n",
      "episode_reward_max: 720.0\n",
      "episode_reward_mean: 375.65\n",
      "episode_reward_min: -20.0\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 440\n",
      "experiment_id: 75b89c0b5aae424298536ee0dbb13c02\n",
      "hostname: debian\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      allreduce_latency: 0.0\n",
      "      cur_kl_coeff: 0.2536541107416499\n",
      "      cur_lr: 5.000000000000001e-05\n",
      "      entropy: 0.9954563634736198\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.022274833704744066\n",
      "      policy_loss: -0.00725916214287281\n",
      "      total_loss: 2137.948556082589\n",
      "      vf_explained_var: 0.9000959396362305\n",
      "      vf_loss: 2137.950177873884\n",
      "  num_steps_sampled: 158800\n",
      "  num_steps_trained: 158800\n",
      "iterations_since_restore: 397\n",
      "node_ip: 192.168.0.3\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 69.45545454545454\n",
      "  ram_util_percent: 59.06636363636361\n",
      "pid: 8220\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06759220666519924\n",
      "  mean_env_wait_ms: 4.030578943213966\n",
      "  mean_inference_ms: 7.518757713971385\n",
      "  mean_raw_obs_processing_ms: 0.24785927950003916\n",
      "time_since_restore: 23113.01966571808\n",
      "time_this_iter_s: 76.88484644889832\n",
      "time_total_s: 23113.01966571808\n",
      "timers:\n",
      "  learn_throughput: 6.66\n",
      "  learn_time_ms: 60056.22\n",
      "  sample_throughput: 74.872\n",
      "  sample_time_ms: 5342.467\n",
      "  update_time_ms: 6.04\n",
      "timestamp: 1612657273\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 158800\n",
      "training_iteration: 397\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2021-02-06_21-22-31\n",
      "done: false\n",
      "episode_len_mean: 368.94\n",
      "episode_reward_max: 720.0\n",
      "episode_reward_mean: 381.65\n",
      "episode_reward_min: -20.0\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 441\n",
      "experiment_id: 75b89c0b5aae424298536ee0dbb13c02\n",
      "hostname: debian\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      allreduce_latency: 0.0\n",
      "      cur_kl_coeff: 0.3804811661124749\n",
      "      cur_lr: 5.000000000000001e-05\n",
      "      entropy: 0.9701735121863229\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.005639520074640002\n",
      "      policy_loss: -0.03079660715801375\n",
      "      total_loss: 2352.41641671317\n",
      "      vf_explained_var: 0.8760861754417419\n",
      "      vf_loss: 2352.445138113839\n",
      "  num_steps_sampled: 159200\n",
      "  num_steps_trained: 159200\n",
      "iterations_since_restore: 398\n",
      "node_ip: 192.168.0.3\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 70.38828828828828\n",
      "  ram_util_percent: 58.86396396396397\n",
      "pid: 8220\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06759937474486526\n",
      "  mean_env_wait_ms: 4.031033015252774\n",
      "  mean_inference_ms: 7.520192453733893\n",
      "  mean_raw_obs_processing_ms: 0.2478989661303223\n",
      "time_since_restore: 23190.983043193817\n",
      "time_this_iter_s: 77.96337747573853\n",
      "time_total_s: 23190.983043193817\n",
      "timers:\n",
      "  learn_throughput: 6.529\n",
      "  learn_time_ms: 61267.174\n",
      "  sample_throughput: 73.231\n",
      "  sample_time_ms: 5462.176\n",
      "  update_time_ms: 6.44\n",
      "timestamp: 1612657351\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 159200\n",
      "training_iteration: 398\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2021-02-06_21-23-51\n",
      "done: false\n",
      "episode_len_mean: 368.98\n",
      "episode_reward_max: 720.0\n",
      "episode_reward_mean: 381.65\n",
      "episode_reward_min: -20.0\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 442\n",
      "experiment_id: 75b89c0b5aae424298536ee0dbb13c02\n",
      "hostname: debian\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      allreduce_latency: 0.0\n",
      "      cur_kl_coeff: 0.3804811661124749\n",
      "      cur_lr: 5.000000000000001e-05\n",
      "      entropy: 0.9729859232902527\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.004788863638948117\n",
      "      policy_loss: -0.02213688407625471\n",
      "      total_loss: 960.3921726771763\n",
      "      vf_explained_var: 0.9245899319648743\n",
      "      vf_loss: 960.4125279017857\n",
      "  num_steps_sampled: 159600\n",
      "  num_steps_trained: 159600\n",
      "iterations_since_restore: 399\n",
      "node_ip: 192.168.0.3\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 68.7061403508772\n",
      "  ram_util_percent: 58.82543859649122\n",
      "pid: 8220\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0676069465540895\n",
      "  mean_env_wait_ms: 4.031506465855798\n",
      "  mean_inference_ms: 7.5217012308212965\n",
      "  mean_raw_obs_processing_ms: 0.24794082389264588\n",
      "time_since_restore: 23270.8146212101\n",
      "time_this_iter_s: 79.83157801628113\n",
      "time_total_s: 23270.8146212101\n",
      "timers:\n",
      "  learn_throughput: 6.398\n",
      "  learn_time_ms: 62521.786\n",
      "  sample_throughput: 71.768\n",
      "  sample_time_ms: 5573.522\n",
      "  update_time_ms: 6.293\n",
      "timestamp: 1612657431\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 159600\n",
      "training_iteration: 399\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2021-02-06_21-25-08\n",
      "done: false\n",
      "episode_len_mean: 368.81\n",
      "episode_reward_max: 720.0\n",
      "episode_reward_mean: 387.25\n",
      "episode_reward_min: -20.0\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 443\n",
      "experiment_id: 75b89c0b5aae424298536ee0dbb13c02\n",
      "hostname: debian\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      allreduce_latency: 0.0\n",
      "      cur_kl_coeff: 0.19024058305623745\n",
      "      cur_lr: 5.000000000000001e-05\n",
      "      entropy: 1.1407902070454188\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.011895614676177502\n",
      "      policy_loss: -0.0374979523143598\n",
      "      total_loss: 3438.2695661272323\n",
      "      vf_explained_var: 0.772524356842041\n",
      "      vf_loss: 3438.3048967633927\n",
      "  num_steps_sampled: 160000\n",
      "  num_steps_trained: 160000\n",
      "iterations_since_restore: 400\n",
      "node_ip: 192.168.0.3\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 66.35585585585585\n",
      "  ram_util_percent: 59.00090090090089\n",
      "pid: 8220\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06761506649579746\n",
      "  mean_env_wait_ms: 4.032000381144378\n",
      "  mean_inference_ms: 7.523300184883905\n",
      "  mean_raw_obs_processing_ms: 0.24798499063830387\n",
      "time_since_restore: 23348.02693080902\n",
      "time_this_iter_s: 77.21230959892273\n",
      "time_total_s: 23348.02693080902\n",
      "timers:\n",
      "  learn_throughput: 6.295\n",
      "  learn_time_ms: 63540.752\n",
      "  sample_throughput: 70.45\n",
      "  sample_time_ms: 5677.765\n",
      "  update_time_ms: 5.902\n",
      "timestamp: 1612657508\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 160000\n",
      "training_iteration: 400\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2021-02-06_21-26-22\n",
      "done: false\n",
      "episode_len_mean: 369.19\n",
      "episode_reward_max: 720.0\n",
      "episode_reward_mean: 387.55\n",
      "episode_reward_min: -20.0\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 444\n",
      "experiment_id: 75b89c0b5aae424298536ee0dbb13c02\n",
      "hostname: debian\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      allreduce_latency: 0.0\n",
      "      cur_kl_coeff: 0.19024058305623745\n",
      "      cur_lr: 5.000000000000001e-05\n",
      "      entropy: 1.0963263171059745\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.008317985305828708\n",
      "      policy_loss: -0.01638857780822686\n",
      "      total_loss: 1223.462411063058\n",
      "      vf_explained_var: 0.9149670004844666\n",
      "      vf_loss: 1223.4772077287946\n",
      "  num_steps_sampled: 160400\n",
      "  num_steps_trained: 160400\n",
      "iterations_since_restore: 401\n",
      "node_ip: 192.168.0.3\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 64.27641509433963\n",
      "  ram_util_percent: 58.985849056603776\n",
      "pid: 8220\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06762372182341625\n",
      "  mean_env_wait_ms: 4.032517897473081\n",
      "  mean_inference_ms: 7.5250091599961\n",
      "  mean_raw_obs_processing_ms: 0.24803243912012288\n",
      "time_since_restore: 23422.335732460022\n",
      "time_this_iter_s: 74.30880165100098\n",
      "time_total_s: 23422.335732460022\n",
      "timers:\n",
      "  learn_throughput: 6.206\n",
      "  learn_time_ms: 64451.805\n",
      "  sample_throughput: 68.761\n",
      "  sample_time_ms: 5817.237\n",
      "  update_time_ms: 5.956\n",
      "timestamp: 1612657582\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 160400\n",
      "training_iteration: 401\n",
      "\n",
      "checkpoint salvo em /home/nelson/ray_results/PPO_MegaMan2-Nes_2021-02-06_14-55-43pae6ulzr/checkpoint_401/checkpoint-401\n",
      "custom_metrics: {}\n",
      "date: 2021-02-06_21-27-34\n",
      "done: false\n",
      "episode_len_mean: 369.36\n",
      "episode_reward_max: 720.0\n",
      "episode_reward_mean: 387.55\n",
      "episode_reward_min: -20.0\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 445\n",
      "experiment_id: 75b89c0b5aae424298536ee0dbb13c02\n",
      "hostname: debian\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      allreduce_latency: 0.0\n",
      "      cur_kl_coeff: 0.19024058305623745\n",
      "      cur_lr: 5.000000000000001e-05\n",
      "      entropy: 1.2030880621501379\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.017226664748575007\n",
      "      policy_loss: -0.03810652026108333\n",
      "      total_loss: 2282.6557965959823\n",
      "      vf_explained_var: 0.8801182508468628\n",
      "      vf_loss: 2282.6906215122767\n",
      "  num_steps_sampled: 160800\n",
      "  num_steps_trained: 160800\n",
      "iterations_since_restore: 402\n",
      "node_ip: 192.168.0.3\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 60.568627450980394\n",
      "  ram_util_percent: 59.14411764705882\n",
      "pid: 8220\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06763273282833036\n",
      "  mean_env_wait_ms: 4.033051436390424\n",
      "  mean_inference_ms: 7.526790349373212\n",
      "  mean_raw_obs_processing_ms: 0.24808183393612704\n",
      "time_since_restore: 23494.171709537506\n",
      "time_this_iter_s: 71.83597707748413\n",
      "time_total_s: 23494.171709537506\n",
      "timers:\n",
      "  learn_throughput: 6.094\n",
      "  learn_time_ms: 65639.479\n",
      "  sample_throughput: 67.049\n",
      "  sample_time_ms: 5965.766\n",
      "  update_time_ms: 6.16\n",
      "timestamp: 1612657654\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 160800\n",
      "training_iteration: 402\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2021-02-06_21-28-45\n",
      "done: false\n",
      "episode_len_mean: 368.3\n",
      "episode_reward_max: 720.0\n",
      "episode_reward_mean: 393.35\n",
      "episode_reward_min: -20.0\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 447\n",
      "experiment_id: 75b89c0b5aae424298536ee0dbb13c02\n",
      "hostname: debian\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      allreduce_latency: 0.0\n",
      "      cur_kl_coeff: 0.19024058305623745\n",
      "      cur_lr: 5.000000000000001e-05\n",
      "      entropy: 1.3006287131990706\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.04771290719509125\n",
      "      policy_loss: -0.09305178746581078\n",
      "      total_loss: 4880.78337751116\n",
      "      vf_explained_var: 0.9134036898612976\n",
      "      vf_loss: 4880.867222377232\n",
      "  num_steps_sampled: 161200\n",
      "  num_steps_trained: 161200\n",
      "iterations_since_restore: 403\n",
      "node_ip: 192.168.0.3\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 57.60099999999999\n",
      "  ram_util_percent: 58.756\n",
      "pid: 8220\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0676513439576369\n",
      "  mean_env_wait_ms: 4.034139142016777\n",
      "  mean_inference_ms: 7.530461152762897\n",
      "  mean_raw_obs_processing_ms: 0.24818298136153205\n",
      "time_since_restore: 23564.316605567932\n",
      "time_this_iter_s: 70.14489603042603\n",
      "time_total_s: 23564.316605567932\n",
      "timers:\n",
      "  learn_throughput: 5.991\n",
      "  learn_time_ms: 66761.974\n",
      "  sample_throughput: 65.671\n",
      "  sample_time_ms: 6091.01\n",
      "  update_time_ms: 6.613\n",
      "timestamp: 1612657725\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 161200\n",
      "training_iteration: 403\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2021-02-06_21-29-56\n",
      "done: false\n",
      "episode_len_mean: 368.3\n",
      "episode_reward_max: 720.0\n",
      "episode_reward_mean: 393.35\n",
      "episode_reward_min: -20.0\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 447\n",
      "experiment_id: 75b89c0b5aae424298536ee0dbb13c02\n",
      "hostname: debian\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      allreduce_latency: 0.0\n",
      "      cur_kl_coeff: 0.2853608745843561\n",
      "      cur_lr: 5.000000000000001e-05\n",
      "      entropy: 1.36416528906141\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.020875747182539532\n",
      "      policy_loss: -0.04266537353396416\n",
      "      total_loss: 4874.320556640625\n",
      "      vf_explained_var: 0.2677616477012634\n",
      "      vf_loss: 4874.357177734375\n",
      "  num_steps_sampled: 161600\n",
      "  num_steps_trained: 161600\n",
      "iterations_since_restore: 404\n",
      "node_ip: 192.168.0.3\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 58.964705882352945\n",
      "  ram_util_percent: 59.12254901960783\n",
      "pid: 8220\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0676513439576369\n",
      "  mean_env_wait_ms: 4.034139142016778\n",
      "  mean_inference_ms: 7.530461152762897\n",
      "  mean_raw_obs_processing_ms: 0.2481829813615321\n",
      "time_since_restore: 23635.53181529045\n",
      "time_this_iter_s: 71.21520972251892\n",
      "time_total_s: 23635.53181529045\n",
      "timers:\n",
      "  learn_throughput: 5.896\n",
      "  learn_time_ms: 67844.616\n",
      "  sample_throughput: 64.415\n",
      "  sample_time_ms: 6209.77\n",
      "  update_time_ms: 7.523\n",
      "timestamp: 1612657796\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 161600\n",
      "training_iteration: 404\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2021-02-06_21-31-10\n",
      "done: false\n",
      "episode_len_mean: 368.56\n",
      "episode_reward_max: 720.0\n",
      "episode_reward_mean: 392.95\n",
      "episode_reward_min: -20.0\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 448\n",
      "experiment_id: 75b89c0b5aae424298536ee0dbb13c02\n",
      "hostname: debian\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      allreduce_latency: 0.0\n",
      "      cur_kl_coeff: 0.4280413118765342\n",
      "      cur_lr: 5.000000000000001e-05\n",
      "      entropy: 1.3554411615644182\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.009774330471243178\n",
      "      policy_loss: -0.025068727987153188\n",
      "      total_loss: 7203.434988839285\n",
      "      vf_explained_var: -1.0\n",
      "      vf_loss: 7203.456263950893\n",
      "  num_steps_sampled: 162000\n",
      "  num_steps_trained: 162000\n",
      "iterations_since_restore: 405\n",
      "node_ip: 192.168.0.3\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 59.28584905660377\n",
      "  ram_util_percent: 59.35188679245283\n",
      "pid: 8220\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06766139548756346\n",
      "  mean_env_wait_ms: 4.034709620732569\n",
      "  mean_inference_ms: 7.5324304261973305\n",
      "  mean_raw_obs_processing_ms: 0.24823743498951445\n",
      "time_since_restore: 23709.566873550415\n",
      "time_this_iter_s: 74.03505825996399\n",
      "time_total_s: 23709.566873550415\n",
      "timers:\n",
      "  learn_throughput: 5.817\n",
      "  learn_time_ms: 68766.8\n",
      "  sample_throughput: 63.163\n",
      "  sample_time_ms: 6332.795\n",
      "  update_time_ms: 7.39\n",
      "timestamp: 1612657870\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 162000\n",
      "training_iteration: 405\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2021-02-06_21-32-25\n",
      "done: false\n",
      "episode_len_mean: 368.42\n",
      "episode_reward_max: 720.0\n",
      "episode_reward_mean: 392.55\n",
      "episode_reward_min: -20.0\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 449\n",
      "experiment_id: 75b89c0b5aae424298536ee0dbb13c02\n",
      "hostname: debian\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      allreduce_latency: 0.0\n",
      "      cur_kl_coeff: 0.4280413118765342\n",
      "      cur_lr: 5.000000000000001e-05\n",
      "      entropy: 1.3026863847460066\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.019318321586719582\n",
      "      policy_loss: -0.0698275523526328\n",
      "      total_loss: 4524.566789899553\n",
      "      vf_explained_var: -0.8523980975151062\n",
      "      vf_loss: 4524.62841796875\n",
      "  num_steps_sampled: 162400\n",
      "  num_steps_trained: 162400\n",
      "iterations_since_restore: 406\n",
      "node_ip: 192.168.0.3\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 63.381308411214945\n",
      "  ram_util_percent: 59.65140186915889\n",
      "pid: 8220\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06767165343532151\n",
      "  mean_env_wait_ms: 4.0352915335280075\n",
      "  mean_inference_ms: 7.53444612981442\n",
      "  mean_raw_obs_processing_ms: 0.2482939703840386\n",
      "time_since_restore: 23785.11783003807\n",
      "time_this_iter_s: 75.55095648765564\n",
      "time_total_s: 23785.11783003807\n",
      "timers:\n",
      "  learn_throughput: 5.842\n",
      "  learn_time_ms: 68474.715\n",
      "  sample_throughput: 62.447\n",
      "  sample_time_ms: 6405.411\n",
      "  update_time_ms: 6.907\n",
      "timestamp: 1612657945\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 162400\n",
      "training_iteration: 406\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2021-02-06_21-33-43\n",
      "done: false\n",
      "episode_len_mean: 367.67\n",
      "episode_reward_max: 720.0\n",
      "episode_reward_mean: 391.55\n",
      "episode_reward_min: -20.0\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 450\n",
      "experiment_id: 75b89c0b5aae424298536ee0dbb13c02\n",
      "hostname: debian\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      allreduce_latency: 0.0\n",
      "      cur_kl_coeff: 0.4280413118765342\n",
      "      cur_lr: 5.000000000000001e-05\n",
      "      entropy: 1.3061144692557198\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.025482647387044772\n",
      "      policy_loss: -0.04770277946123055\n",
      "      total_loss: 1031.2804652622767\n",
      "      vf_explained_var: 0.48085424304008484\n",
      "      vf_loss: 1031.3172694614955\n",
      "  num_steps_sampled: 162800\n",
      "  num_steps_trained: 162800\n",
      "iterations_since_restore: 407\n",
      "node_ip: 192.168.0.3\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 66.55225225225225\n",
      "  ram_util_percent: 59.472972972972975\n",
      "pid: 8220\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06768231338277825\n",
      "  mean_env_wait_ms: 4.0358905293555685\n",
      "  mean_inference_ms: 7.5365450855791565\n",
      "  mean_raw_obs_processing_ms: 0.24835364042445962\n",
      "time_since_restore: 23862.88129925728\n",
      "time_this_iter_s: 77.76346921920776\n",
      "time_total_s: 23862.88129925728\n",
      "timers:\n",
      "  learn_throughput: 5.833\n",
      "  learn_time_ms: 68579.458\n",
      "  sample_throughput: 62.606\n",
      "  sample_time_ms: 6389.126\n",
      "  update_time_ms: 6.695\n",
      "timestamp: 1612658023\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 162800\n",
      "training_iteration: 407\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2021-02-06_21-35-06\n",
      "done: false\n",
      "episode_len_mean: 367.55\n",
      "episode_reward_max: 720.0\n",
      "episode_reward_mean: 391.35\n",
      "episode_reward_min: -20.0\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 451\n",
      "experiment_id: 75b89c0b5aae424298536ee0dbb13c02\n",
      "hostname: debian\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      allreduce_latency: 0.0\n",
      "      cur_kl_coeff: 0.6420619678148013\n",
      "      cur_lr: 5.000000000000001e-05\n",
      "      entropy: 1.2630095141274589\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.00960328203759023\n",
      "      policy_loss: -0.03459656451429639\n",
      "      total_loss: 1736.8853585379463\n",
      "      vf_explained_var: 0.4866294860839844\n",
      "      vf_loss: 1736.9138357979912\n",
      "  num_steps_sampled: 163200\n",
      "  num_steps_trained: 163200\n",
      "iterations_since_restore: 408\n",
      "node_ip: 192.168.0.3\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 75.73025210084032\n",
      "  ram_util_percent: 61.16134453781512\n",
      "pid: 8220\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06769339578414689\n",
      "  mean_env_wait_ms: 4.036508686259595\n",
      "  mean_inference_ms: 7.538726725742552\n",
      "  mean_raw_obs_processing_ms: 0.24841602207181118\n",
      "time_since_restore: 23945.732045412064\n",
      "time_this_iter_s: 82.85074615478516\n",
      "time_total_s: 23945.732045412064\n",
      "timers:\n",
      "  learn_throughput: 5.788\n",
      "  learn_time_ms: 69107.22\n",
      "  sample_throughput: 62.998\n",
      "  sample_time_ms: 6349.36\n",
      "  update_time_ms: 7.861\n",
      "timestamp: 1612658106\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 163200\n",
      "training_iteration: 408\n",
      "\n",
      "custom_metrics: {}\n",
      "date: 2021-02-06_21-36-36\n",
      "done: false\n",
      "episode_len_mean: 368.37\n",
      "episode_reward_max: 720.0\n",
      "episode_reward_mean: 385.4\n",
      "episode_reward_min: -20.0\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 452\n",
      "experiment_id: 75b89c0b5aae424298536ee0dbb13c02\n",
      "hostname: debian\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      allreduce_latency: 0.0\n",
      "      cur_kl_coeff: 0.6420619678148013\n",
      "      cur_lr: 5.000000000000001e-05\n",
      "      entropy: 1.3176421437944685\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.012401304739926542\n",
      "      policy_loss: -0.02227636213813509\n",
      "      total_loss: 7268.334751674107\n",
      "      vf_explained_var: -0.17594337463378906\n",
      "      vf_loss: 7268.349190848215\n",
      "  num_steps_sampled: 163600\n",
      "  num_steps_trained: 163600\n",
      "iterations_since_restore: 409\n",
      "node_ip: 192.168.0.3\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 78.70551181102364\n",
      "  ram_util_percent: 61.60393700787402\n",
      "pid: 8220\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06770505411029888\n",
      "  mean_env_wait_ms: 4.037158443033856\n",
      "  mean_inference_ms: 7.541014894549271\n",
      "  mean_raw_obs_processing_ms: 0.24848209721244824\n",
      "time_since_restore: 24035.09161543846\n",
      "time_this_iter_s: 89.3595700263977\n",
      "time_total_s: 24035.09161543846\n",
      "timers:\n",
      "  learn_throughput: 5.714\n",
      "  learn_time_ms: 69997.664\n",
      "  sample_throughput: 62.422\n",
      "  sample_time_ms: 6408.033\n",
      "  update_time_ms: 10.845\n",
      "timestamp: 1612658196\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 163600\n",
      "training_iteration: 409\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-52d10334c86e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MegaMan2-Nes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-74b0d534eae1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, config, checkpoint, iterations)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMAX_WORKER_FAILURE_RETRIES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRayError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ignore_worker_failures\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/ray/tune/trainable.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \"\"\"\n\u001b[1;32m    182\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"step() needs to return a dict.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_exec_impl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_filter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"LocalIterator[T]\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mapply_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_filter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"LocalIterator[T]\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mapply_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mapply_foreach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    784\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/ray/util/iter.py\u001b[0m in \u001b[0;36mapply_foreach\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    789\u001b[0m                         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m                             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m                                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m                             \u001b[0;32myield\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_NextValueNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/ray/rllib/execution/train_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_sgd_iter\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgd_minibatch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 info = do_minibatch_sgd(\n\u001b[0m\u001b[1;32m     63\u001b[0m                     batch, {p: w.get_policy(p)\n\u001b[1;32m     64\u001b[0m                             for p in self.policies}, w, self.num_sgd_iter,\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/ray/rllib/utils/sgd.py\u001b[0m in \u001b[0;36mdo_minibatch_sgd\u001b[0;34m(samples, policies, local_worker, num_sgd_iter, sgd_minibatch_size, standardize_fields)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0miter_extra_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd_minibatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 batch_fetches = (local_worker.learn_on_batch(\n\u001b[0m\u001b[1;32m    129\u001b[0m                     MultiAgentBatch({\n\u001b[1;32m    130\u001b[0m                         \u001b[0mpolicy_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\u001b[0m in \u001b[0;36mlearn_on_batch\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    825\u001b[0m                         builder, batch)\n\u001b[1;32m    826\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m                     \u001b[0minfo_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m             \u001b[0minfo_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto_fetch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py\u001b[0m in \u001b[0;36mlearn_on_batch\u001b[0;34m(self, postprocessed_batch)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;31m# Compute gradients (will calculate all losses and `backward()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# them to get the grads).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpostprocessed_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;31m# Step the optimizers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, postprocessed_batch)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# Recompute gradients of loss over all variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0mloss_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m             \u001b[0mgrad_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_grad_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = train(\"MegaMan2-Nes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(config=trainer_config, checkpoint=None, testdelay=0, render=False, envcreator=None, maxepisodelen=10000000):\n",
    "    \"\"\"Tests and renders a previously trained model\"\"\"\n",
    "\n",
    "    agent = PPOTrainer(config=config, env='MegaMan2-Nes')\n",
    "    if checkpoint is None:\n",
    "        raise ValEuerror(f\"A previously trained checkpoint must be provided for algorithm {alg}\")\n",
    "    agent.restore(checkpoint)\n",
    "\n",
    "    game_rom = \"MegaMan2-Nes\" #Nome da rom\n",
    "    state = \"Normal.Metalman.Fight.state\" \n",
    "    scenario = \"scenario\"\n",
    "    env = retro.make(game_rom, state=state, scenario=scenario)\n",
    "    env = retro_wrappers.wrap_megaman(env)\n",
    "\n",
    "    while True:\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        reward_total = 0.0\n",
    "        step = 0\n",
    "        while not done and step < maxepisodelen:\n",
    "            action = agent.compute_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            reward_total += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            state = next_state\n",
    "            step = step + 1\n",
    "        print(\"Episode reward\", reward_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-02-06 22:17:22,574\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "2021-02-06 22:17:22,661\tINFO trainable.py:328 -- Restored on 192.168.0.3 from checkpoint: /home/nelson/ray_results/PPO_MegaMan2-Nes_2021-02-06_14-55-43pae6ulzr/checkpoint_401/checkpoint-401\n",
      "2021-02-06 22:17:22,666\tINFO trainable.py:336 -- Current state after restoring: {'_iteration': 401, '_timesteps_total': None, '_time_total': 23422.335732460022, '_episodes_total': 444}\n",
      "Episode reward 675.0\n",
      "Episode reward 675.0\n",
      "Episode reward 660.0\n",
      "Episode reward 100.0\n",
      "Episode reward 660.0\n",
      "Episode reward 645.0\n",
      "Episode reward 690.0\n",
      "Episode reward 645.0\n",
      "Episode reward 675.0\n",
      "Episode reward 675.0\n",
      "Episode reward 690.0\n",
      "Episode reward 660.0\n",
      "Episode reward 660.0\n",
      "Episode reward 100.0\n",
      "Episode reward 675.0\n",
      "Episode reward 100.0\n",
      "Episode reward 675.0\n",
      "Episode reward 640.0\n",
      "Episode reward 100.0\n",
      "Episode reward 60.0\n",
      "Episode reward 120.0\n",
      "Episode reward 120.0\n",
      "Episode reward 660.0\n",
      "Episode reward 690.0\n",
      "Episode reward 120.0\n",
      "Episode reward 690.0\n",
      "Episode reward 120.0\n",
      "Episode reward 675.0\n",
      "Episode reward 645.0\n",
      "Episode reward 660.0\n",
      "Episode reward 675.0\n",
      "Episode reward 660.0\n",
      "Episode reward 660.0\n",
      "Episode reward 120.0\n",
      "Episode reward 80.0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-ad06ee78703b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/nelson/ray_results/PPO_MegaMan2-Nes_2021-02-06_14-55-43pae6ulzr/checkpoint_401/checkpoint-401\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-4414431d0fe9>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(config, checkpoint, testdelay, render, envcreator, maxepisodelen)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmaxepisodelen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mreward_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/eps/Turing/RL/TuringRetro/utils/retro_wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/retro/lib/python3.8/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/eps/Turing/RL/TuringRetro/utils/retro_wrappers.py\u001b[0m in \u001b[0;36mobservation\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grayscale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_RGB2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         frame = cv2.resize(\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "s = \"/home/nelson/ray_results/PPO_MegaMan2-Nes_2021-02-06_14-55-43pae6ulzr/checkpoint_401/checkpoint-401\"\n",
    "test(checkpoint=s, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-1baceacf4cb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}